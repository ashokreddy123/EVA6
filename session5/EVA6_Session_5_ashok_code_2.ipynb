{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EVA6 - Session 5_ashok_code_2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "qucKZlGbrV5H"
      },
      "source": [
        "from __future__ import print_function\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLTzh2FtBHPE"
      },
      "source": [
        "# Train Phase transformations\n",
        "train_transforms = transforms.Compose([\n",
        "                                      #  transforms.Resize((28, 28)),\n",
        "                                      #  transforms.ColorJitter(brightness=0.10, contrast=0.1, saturation=0.10, hue=0.1),\n",
        "                                       transforms.ToTensor(),\n",
        "                                       transforms.Normalize((0.1307,), (0.3081,)) # The mean and std have to be sequences (e.g., tuples), therefore you should add a comma after the values. \n",
        "                                       # Note the difference between (0.1307) and (0.1307,)\n",
        "                                       ])\n",
        "\n",
        "# Test Phase transformations\n",
        "test_transforms = transforms.Compose([\n",
        "                                      #  transforms.Resize((28, 28)),\n",
        "                                      #  transforms.ColorJitter(brightness=0.10, contrast=0.1, saturation=0.10, hue=0.1),\n",
        "                                       transforms.ToTensor(),\n",
        "                                       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                                       ])\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xsVOPwMmBJtq"
      },
      "source": [
        "train = datasets.MNIST('./data', train=True, download=True, transform=train_transforms)\n",
        "test = datasets.MNIST('./data', train=False, download=True, transform=test_transforms)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GWZdk1s7BNpK",
        "outputId": "d825c38d-7ce4-4b16-e33c-09857bab1f49"
      },
      "source": [
        "SEED = 1\n",
        "\n",
        "# CUDA?\n",
        "cuda = torch.cuda.is_available()\n",
        "print(\"CUDA Available?\", cuda)\n",
        "\n",
        "# For reproducibility\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "if cuda:\n",
        "    torch.cuda.manual_seed(SEED)\n",
        "\n",
        "# dataloader arguments - something you'll fetch these from cmdprmt\n",
        "dataloader_args = dict(shuffle=True, batch_size=128, num_workers=4, pin_memory=True) if cuda else dict(shuffle=True, batch_size=64)\n",
        "\n",
        "# train dataloader\n",
        "train_loader = torch.utils.data.DataLoader(train, **dataloader_args)\n",
        "\n",
        "# test dataloader\n",
        "test_loader = torch.utils.data.DataLoader(test, **dataloader_args)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CUDA Available? True\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PzNo9EhKBi7q"
      },
      "source": [
        "\n",
        "class Net(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(Net, self).__init__()\n",
        "    \n",
        "    #convolutuonla block 1\n",
        "    self.convblock1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=1, out_channels=8, kernel_size=(3, 3), padding=0, bias=False),\n",
        "            nn.ReLU()\n",
        "    \n",
        "    ) #input_size = 28, #output_size = 26, RF = 3\n",
        "    self.convblock2 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=8, out_channels=16, kernel_size=(3, 3), padding=0, bias=False),\n",
        "            nn.ReLU()\n",
        "\n",
        "    ) #input_size = 26, #output_size = 24, RF = 5\n",
        "    \"\"\"\n",
        "    self.convblock3 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=20, out_channels=20, kernel_size=(3, 3), padding=0, bias=False),\n",
        "            nn.BatchNorm2d(20),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(drop_out)\n",
        "    ) #output_size = 22\n",
        "   \"\"\"\n",
        "    # transition block\n",
        "    self.maxpool1 = nn.MaxPool2d(2,2) #input_size = 24, #output_size = 12, RF = 6\n",
        "    self.convblock4 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=16, out_channels=8, kernel_size=(1, 1), padding=0, bias=False),\n",
        "            nn.ReLU()\n",
        "           \n",
        "    )#input_size = 12, #output_size = 10, RF = 10\n",
        "\n",
        "    #convolutuonla block 2\n",
        "    self.convblock5 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=8, out_channels=8, kernel_size=(3, 3), padding=0, bias=False),\n",
        "            nn.ReLU()\n",
        "        \n",
        "    )#input_size = 10, #output_size = 8, RF = 14\n",
        "    self.convblock6 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=8, out_channels=16, kernel_size=(3, 3), padding=0, bias=False),\n",
        "            nn.ReLU()\n",
        "    )#input_size = 8, #output_size = 6, RF = 18\n",
        "    self.convblock7 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=16, out_channels=16, kernel_size=(3, 3), padding=0, bias=False),\n",
        "            nn.ReLU()\n",
        "    )#input_size = 6, #output_size = 4, RF = 22\n",
        "    self.convblock8 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=16, out_channels=16, kernel_size=(3, 3), padding=0, bias=False),\n",
        "            nn.ReLU()\n",
        "        \n",
        "    )#input_size = 4, #output_size = 2, RF = 26\n",
        "\n",
        "    #output block\n",
        "    self.gap = nn.AdaptiveAvgPool2d((1,1)) #input_size = 2, #output_size = 1, RF = 28\n",
        "    self.convblock9 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=16, out_channels=10, kernel_size=(1,1), padding=0, bias=False),\n",
        "            \n",
        "    )\n",
        "\n",
        "  def forward(self,input):\n",
        "\n",
        "    #x = self.convblock3(self.convblock2(self.convblock1(input)))\n",
        "    x = self.convblock2(self.convblock1(input))\n",
        "\n",
        "    x = self.convblock4(self.maxpool1(x))\n",
        "\n",
        "    x = self.convblock8(self.convblock7(self.convblock6(self.convblock5(x))))\n",
        "\n",
        "    x = self.convblock9(self.gap(x))\n",
        "\n",
        "    return F.log_softmax(x)\n",
        "\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fj-oPhGTBlT7",
        "outputId": "f073117c-26c7-40e5-dea6-40ee6e00e357"
      },
      "source": [
        "!pip install torchsummary\n",
        "from torchsummary import summary\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "print(device)\n",
        "model = Net().to(device)\n",
        "summary(model, input_size=(1, 28, 28))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.7/dist-packages (1.5.1)\n",
            "cuda\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1            [-1, 8, 26, 26]              72\n",
            "              ReLU-2            [-1, 8, 26, 26]               0\n",
            "            Conv2d-3           [-1, 16, 24, 24]           1,152\n",
            "              ReLU-4           [-1, 16, 24, 24]               0\n",
            "         MaxPool2d-5           [-1, 16, 12, 12]               0\n",
            "            Conv2d-6            [-1, 8, 12, 12]             128\n",
            "              ReLU-7            [-1, 8, 12, 12]               0\n",
            "            Conv2d-8            [-1, 8, 10, 10]             576\n",
            "              ReLU-9            [-1, 8, 10, 10]               0\n",
            "           Conv2d-10             [-1, 16, 8, 8]           1,152\n",
            "             ReLU-11             [-1, 16, 8, 8]               0\n",
            "           Conv2d-12             [-1, 16, 6, 6]           2,304\n",
            "             ReLU-13             [-1, 16, 6, 6]               0\n",
            "           Conv2d-14             [-1, 16, 4, 4]           2,304\n",
            "             ReLU-15             [-1, 16, 4, 4]               0\n",
            "AdaptiveAvgPool2d-16             [-1, 16, 1, 1]               0\n",
            "           Conv2d-17             [-1, 10, 1, 1]             160\n",
            "================================================================\n",
            "Total params: 7,848\n",
            "Trainable params: 7,848\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.30\n",
            "Params size (MB): 0.03\n",
            "Estimated Total Size (MB): 0.33\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:72: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRdapvWgBtVE"
      },
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "train_acc = []\n",
        "test_acc = []\n",
        "\n",
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "  model.train()\n",
        "  #pbar = tqdm(train_loader)\n",
        "  correct = 0\n",
        "  processed = 0\n",
        "  for batch_idx, (data, target) in enumerate(train_loader):\n",
        "    # get samples\n",
        "    data, target = data.to(device), target.to(device)\n",
        "    #print(\"target size:\",target.size())\n",
        "    #target = target.squeeze(1)\n",
        "    #print(\"target size2:\",target.size())\n",
        "    # Init\n",
        "    optimizer.zero_grad()\n",
        "    # In PyTorch, we need to set the gradients to zero before starting to do backpropragation because PyTorch accumulates the gradients on subsequent backward passes. \n",
        "    # Because of this, when you start your training loop, ideally you should zero out the gradients so that you do the parameter update correctly.\n",
        "\n",
        "    # Predict\n",
        "    y_pred = model(data)\n",
        "    #print(\"y_pred size:\",y_pred.size())\n",
        "    y_pred = y_pred.squeeze(-1)\n",
        "    y_pred = y_pred.squeeze(-1)\n",
        "    #print(\"y_pred size2:\",y_pred.size())\n",
        "    # Calculate loss\n",
        "    loss = F.nll_loss(y_pred, target)\n",
        "    train_losses.append(loss)\n",
        "\n",
        "    # Backpropagation\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Update pbar-tqdm\n",
        "    \n",
        "    pred = y_pred.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "    correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "    processed += len(data)\n",
        "\n",
        "    #pbar.set_description(desc= f'Loss={loss.item()} Batch_id={batch_idx} Accuracy={100*correct/processed:0.2f}')\n",
        "    train_acc.append(100*correct/processed)\n",
        "  return (100*correct/processed)\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            output = output.squeeze(-1)\n",
        "            output = output.squeeze(-1)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    test_losses.append(test_loss)\n",
        "    \n",
        "    \"\"\"\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "    \"\"\"\n",
        "    test_acc.append(100. * correct / len(test_loader.dataset))\n",
        "    return (100. * correct / len(test_loader.dataset))"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0RNneCKBvz3",
        "outputId": "67f730e9-6a72-46fc-8c21-aaa3db5f9765"
      },
      "source": [
        "model =  Net().to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "EPOCHS = 14\n",
        "for epoch in range(EPOCHS):\n",
        "    #print(\"EPOCH:\", epoch)\n",
        "    train_accuracy = train(model, device, train_loader, optimizer, epoch)\n",
        "    test_accuracy = test(model, device, test_loader)\n",
        "    print(\"epoch no: \",epoch,\" train_accuracy: \",train_accuracy,\" test_accuracy: \",test_accuracy)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:72: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch no:  0  train_accuracy:  9.301666666666666  test_accuracy:  10.06\n",
            "epoch no:  1  train_accuracy:  10.531666666666666  test_accuracy:  12.97\n",
            "epoch no:  2  train_accuracy:  28.546666666666667  test_accuracy:  86.03\n",
            "epoch no:  3  train_accuracy:  91.89166666666667  test_accuracy:  95.59\n",
            "epoch no:  4  train_accuracy:  95.8  test_accuracy:  96.06\n",
            "epoch no:  5  train_accuracy:  96.91333333333333  test_accuracy:  97.47\n",
            "epoch no:  6  train_accuracy:  97.38833333333334  test_accuracy:  98.02\n",
            "epoch no:  7  train_accuracy:  97.73166666666667  test_accuracy:  98.24\n",
            "epoch no:  8  train_accuracy:  97.93166666666667  test_accuracy:  98.34\n",
            "epoch no:  9  train_accuracy:  98.14833333333333  test_accuracy:  98.32\n",
            "epoch no:  10  train_accuracy:  98.29833333333333  test_accuracy:  97.92\n",
            "epoch no:  11  train_accuracy:  98.46833333333333  test_accuracy:  98.31\n",
            "epoch no:  12  train_accuracy:  98.52666666666667  test_accuracy:  98.44\n",
            "epoch no:  13  train_accuracy:  98.59  test_accuracy:  98.26\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKAxO1BPttj2"
      },
      "source": [
        "Target :\n",
        "\n",
        "    Bringing the no.of parameters around 8k (required for late students)\n",
        "    Introduction of GAP before output block to reduce parameters\n",
        "    \n",
        "\n",
        "Results :\n",
        "\n",
        "    Parameters - 8,024\n",
        "    Best Train accuracy - 98.59 (within 14 epochs)\n",
        "    Best Test accuracy - 98.44\n",
        "\n",
        "Analysis :\n",
        "\n",
        "    Less overfitting in model\n",
        "    Accuracy is less as the no.of parameters reduced a lot\n",
        "    Model performance can be improved by introducing regularisation techniques"
      ]
    }
  ]
}